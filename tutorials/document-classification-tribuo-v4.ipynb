{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "This tutorial will show how to perform document classification in Tribuo, using a variety of different methods to extract features from the text. We'll use the venerable [20-newsgroups dataset](http://qwone.com/~jason/20Newsgroups/) where the task is to predict what newsgroup a particular post is from, though this tutorial would be equally applicable to any document classification task (including tasks like sentiment analysis). We're going to train a simple logistic regression with fixed hyperparameters using a variety of feature extraction methods. The aim is to show how to extract features from text rather than focusing on the performance, as using a more powerful model like XGBoost, or performing hyperparameter optimization on the logisitic regression will likely improve the performance of all the feature extraction techniques.\n",
    "\n",
    "# Setup\n",
    "\n",
    "You'll need a copy of the 20 newsgroups dataset, so first download and unpack it:\n",
    "\n",
    "```\n",
    "wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n",
    "mkdir 20news\n",
    "cd 20news\n",
    "tar -zxf ../20news-bydate.tar.gz\n",
    "```\n",
    "\n",
    "This leaves you with two directories `20news-bydate-train` and `20news-bydate-test`, which contain the standard train and test split for this data.\n",
    "\n",
    "20 newsgroups comes in a fairly standard format, the dataset is represented by a set of directories where the directory name is the class label, and the directory contains a collection of documents with one document in each file. Each file is a single Usenet post. For the purposes of this tutorial, we'll use the subject and body of the post as the input text for classification.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "$ ls 20news-bydate-train/\n",
    "alt.atheism/               comp.sys.mac.hardware/  rec.motorcycles/     sci.electronics/         talk.politics.guns/\n",
    "comp.graphics/             comp.windows.x/         rec.sport.baseball/  sci.med/                 talk.politics.mideast/\n",
    "comp.os.ms-windows.misc/   misc.forsale/           rec.sport.hockey/    sci.space/               talk.politics.misc/\n",
    "comp.sys.ibm.pc.hardware/  rec.autos/              sci.crypt/           soc.religion.christian/  talk.religion.misc/\n",
    "$ ls 20news-bydate-train/comp.graphics/\n",
    "37261  37949  38233  38270  38305  38344  38381  38417  38454  38489  38525  38562  38598  38633  38668  38703  38739\n",
    "37913  37950  38234  38271  38306  38346  38382  38418  38455  38490  38526  38563  38599  38634  38669  38704  38740\n",
    "37914  37951  38235  38272  38307  38347  38383  38420  38456  38491  38527  38564  38600  38635  38670  38705  38741\n",
    "37915  37952  38236  38273  38308  38348  38384  38421  38457  38492  38528  38565  38601  38636  38671  38706  38742\n",
    "...\n",
    "```\n",
    "\n",
    "As this is a pretty common format, Tribuo has a specific `DataSource` which can be used to read in this sort of data, `org.tribuo.data.text.DirectoryFileSource`.\n",
    "\n",
    "We're going to use the classification experiments jar, along with the ONNX jar which provides support for loading in contextual word embedding models like [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jars ./tribuo-classification-experiments-4.1.0-SNAPSHOT-jar-with-dependencies.jar\n",
    "%jars ./tribuo-onnx-4.1.0-SNAPSHOT-jar-with-dependencies.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a selection of imports from the `org.tribuo.data.text` package, along with the usual imports from `org.tribuo` and `org.tribuo.classification` we use when working with classification tasks. We'll load in the BERT support from the `org.tribuo.interop.onnx.extractors` package. Tribuo's BERT support loads in models and tokenizers from [HuggingFace's Transformer](https://huggingface.co/transformers/) package, and can be easily extended to support non-BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.Collections;\n",
    "import java.nio.file.Paths;\n",
    "import com.oracle.labs.mlrg.olcut.provenance.ProvenanceUtil;\n",
    "import com.oracle.labs.mlrg.olcut.util.Pair;\n",
    "import org.tribuo.*;\n",
    "import org.tribuo.data.text.*;\n",
    "import org.tribuo.data.text.impl.*;\n",
    "import org.tribuo.dataset.MinimumCardinalityDataset;\n",
    "import org.tribuo.classification.*;\n",
    "import org.tribuo.classification.evaluation.*;\n",
    "import org.tribuo.classification.sgd.linear.LinearSGDTrainer;\n",
    "import org.tribuo.classification.sgd.objectives.LogMulticlass;\n",
    "import org.tribuo.interop.onnx.extractors.BERTFeatureExtractor;\n",
    "import org.tribuo.math.optimisers.AdaGrad;\n",
    "import org.tribuo.transform.*;\n",
    "import org.tribuo.transform.transformations.IDFTransformation;\n",
    "import org.tribuo.util.tokens.universal.UniversalTokenizer;\n",
    "import org.tribuo.util.Util;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a few classes that we'll use throughout this tutorial, the label factory, the evaluator and the paths to the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var labelFactory = new LabelFactory();\n",
    "var labelEvaluator = new LabelEvaluator();\n",
    "var trainPath = Paths.get(\".\",\"20news\",\"20news-bydate-train\");\n",
    "var testPath = Paths.get(\".\",\"20news\",\"20news-bydate-test\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text\n",
    "Much of the work of machine learning is in presenting an appropriate representation of the data to the model. This is especially true when working with text data, as there is a plethora of approaches for converting text into the numbers that ML algorithms operate on. The `DirectoryFileSource` allows the user to choose the feature extraction, as it requires a `TextFeatureExtractor` which converts the `String` representing the input text into a Tribuo `Example`. We'll cover several different implementations of the `TextFeatureExtractor` interface in this tutorial, and we expect that users will implement it in their own classes to cope with specific feature extraction requirements.\n",
    "\n",
    "We'll start with the simplest approach, a \"bag of words\", where each document is represented by the counts of the words in that document. This means the feature space is equal to the number of words, and most documents only have a positive value for a small number of words (as most words don't appear in any given document). This is particularly well suited to Tribuo's sparse vector representation of examples, and this suitability for NLP tasks is the reason that Tribuo is designed this way. Of course, first we'll need to tell the extractor what a word is, and for this we use a `Tokenizer`. Tokenizers split up a `String` into a stream of tokens. Tribuo provides several basic tokenizers, and an interface for tokenization. We're going to use Tribuo's `UniversalTokenizer` which is descended from tokenizers developed at Sun Labs in the 90s, and used in a variety of Sun products since that time. First we'll use a strict bag of words where each feature takes the value `1` if that word is present in the document, and `0` otherwise. We'll use Tribuo's `BasicPipeline` which can convert `String`s into features, and pass it to the basic `TextFeatureExtractor` implementation, helpfully called `TextFeatureExtractorImpl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var tokenizer = new UniversalTokenizer();\n",
    "var bowPipeline = new BasicPipeline(tokenizer,1);\n",
    "var bowExtractor = new TextFeatureExtractorImpl<Label>(bowPipeline);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now almost ready to make our train and test data sources, and load in the data. The `DirectoryFileSource` also accepts an array of `DocumentPreprocessor`s which can be used to transform the text before feature extraction takes place. We're going to use a specific preprocessor (`NewsPreprocessor`) which standardises the 20 newsgroups data by stripping out the mail headers and returning only the subject and the body of the email. In general the preprocessors are dataset and task specific, which is why Tribuo doesn't ship with many implementations as in most cases users will need to write one from scratch for their specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "var newsProc = new NewsPreprocessor();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a helper function to load the data sources and create the datasets. We're also going to restrict the test dataset so it only contains valid examples, as 20 newsgroups has some test examples that share no words with the train examples (and so have no features we could use to make predictions with).\n",
    "\n",
    "Let's check our datasets and see if everything has loaded in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow training data size = 11314, number of features = 146037, number of classes = 20\n",
      "bow testing data size = 7531, number of features = 146037, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "public Pair<Dataset<Label>,Dataset<Label>> mkDatasets(String name, TextFeatureExtractor<Label> extractor) {\n",
    "    var trainSource = new DirectoryFileSource<>(trainPath,labelFactory,extractor,newsProc);\n",
    "    var testSource = new DirectoryFileSource<>(testPath,labelFactory,extractor,newsProc);\n",
    "    var trainDS = new MutableDataset<>(trainSource);\n",
    "    var testDS = new ImmutableDataset<>(testSource,trainDS.getFeatureIDMap(),trainDS.getOutputIDInfo(),true);\n",
    "    System.out.println(String.format(name + \" training data size = %d, number of features = %d, number of classes = %d\",trainDS.size(),trainDS.getFeatureMap().size(),trainDS.getOutputInfo().size()));\n",
    "    System.out.println(String.format(name + \" testing data size = %d, number of features = %d, number of classes = %d\",testDS.size(),testDS.getFeatureMap().size(),testDS.getOutputInfo().size()));\n",
    "    return new Pair<>(trainDS,testDS);\n",
    "}\n",
    "\n",
    "var bowPair = mkDatasets(\"bow\",bowExtractor);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded in 11,314 training documents containing 146,037 unique words and 7,532 test documents, each with the expected 20 classes.\n",
    "\n",
    "Now we're ready to train a model. Let's start with a simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on BoW features took (00:00:14:196)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         351          47          91       0.882       0.794       0.836\n",
      "rec.autos                          396         342          54          67       0.864       0.836       0.850\n",
      "talk.religion.misc                 251         170          81         138       0.677       0.552       0.608\n",
      "comp.windows.x                     394         298          96          52       0.756       0.851       0.801\n",
      "rec.sport.baseball                 397         362          35          44       0.912       0.892       0.902\n",
      "comp.graphics                      389         285         104         132       0.733       0.683       0.707\n",
      "talk.politics.mideast              376         278          98           9       0.739       0.969       0.839\n",
      "comp.sys.ibm.pc.hardware           392         283         109         161       0.722       0.637       0.677\n",
      "sci.med                            396         317          79          57       0.801       0.848       0.823\n",
      "comp.os.ms-windows.misc            394         266         128         102       0.675       0.723       0.698\n",
      "sci.crypt                          396         349          47          31       0.881       0.918       0.899\n",
      "comp.sys.mac.hardware              385         282         103          87       0.732       0.764       0.748\n",
      "misc.forsale                       390         345          45          70       0.885       0.831       0.857\n",
      "rec.motorcycles                    398         361          37          31       0.907       0.921       0.914\n",
      "talk.politics.misc                 310         188         122         103       0.606       0.646       0.626\n",
      "sci.electronics                    393         265         128         133       0.674       0.666       0.670\n",
      "rec.sport.hockey                   399         372          27          29       0.932       0.928       0.930\n",
      "sci.space                          394         322          72          46       0.817       0.875       0.845\n",
      "alt.atheism                        319         237          82          65       0.743       0.785       0.763\n",
      "talk.politics.guns                 364         304          60         106       0.835       0.741       0.786\n",
      "Total                            7,531       5,977       1,554       1,554\n",
      "Accuracy                                                                         0.794\n",
      "Micro Average                                                                    0.794       0.794       0.794\n",
      "Macro Average                                                                    0.789       0.793       0.789\n",
      "Balanced Error Rate                                                              0.211\n"
     ]
    }
   ],
   "source": [
    "var lrTrainer = new LinearSGDTrainer(new LogMulticlass(),new AdaGrad(0.1,0.001),5,42);\n",
    "var bowStartTime = System.currentTimeMillis();\n",
    "var bowModel = lrTrainer.train(bowPair.getA());\n",
    "var bowEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on BoW features took \" + Util.formatDuration(bowStartTime,bowEndTime));\n",
    "System.out.println();\n",
    "var bowEval = labelEvaluator.evaluate(bowModel,bowPair.getB());\n",
    "System.out.println(bowEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term counting\n",
    "This approach discards a lot of information about the documents, as we're ignoring how many times the word or n-gram appears in the document (also known in information retrieval circles as the Term Frequency or TF). Let's swap the `BasicPipeline` for a `TokenPipeline` which supports term counting via a constructor flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram training data size = 11314, number of features = 146037, number of classes = 20\n",
      "unigram testing data size = 7531, number of features = 146037, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "var unigramPipeline = new TokenPipeline(tokenizer, 1, true);\n",
    "var unigramExtractor = new TextFeatureExtractorImpl<Label>(unigramPipeline);\n",
    "var unigramPair = mkDatasets(\"unigram\",unigramExtractor);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of documents and number of features are still the same, all that's different is the feature values. Let's build another logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on Unigram features took (00:00:12:181)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         345          53          75       0.867       0.821       0.844\n",
      "rec.autos                          396         351          45          75       0.886       0.824       0.854\n",
      "talk.religion.misc                 251         142         109         102       0.566       0.582       0.574\n",
      "comp.windows.x                     394         298          96          46       0.756       0.866       0.808\n",
      "rec.sport.baseball                 397         355          42          45       0.894       0.888       0.891\n",
      "comp.graphics                      389         277         112         118       0.712       0.701       0.707\n",
      "talk.politics.mideast              376         287          89          19       0.763       0.938       0.842\n",
      "comp.sys.ibm.pc.hardware           392         273         119         138       0.696       0.664       0.680\n",
      "sci.med                            396         308          88          54       0.778       0.851       0.813\n",
      "comp.os.ms-windows.misc            394         256         138          79       0.650       0.764       0.702\n",
      "sci.crypt                          396         355          41          79       0.896       0.818       0.855\n",
      "comp.sys.mac.hardware              385         301          84         100       0.782       0.751       0.766\n",
      "misc.forsale                       390         352          38          75       0.903       0.824       0.862\n",
      "rec.motorcycles                    398         354          44          20       0.889       0.947       0.917\n",
      "talk.politics.misc                 310         179         131         108       0.577       0.624       0.600\n",
      "sci.electronics                    393         289         104         118       0.735       0.710       0.722\n",
      "rec.sport.hockey                   399         374          25          21       0.937       0.947       0.942\n",
      "sci.space                          394         331          63          51       0.840       0.866       0.853\n",
      "alt.atheism                        319         237          82          90       0.743       0.725       0.734\n",
      "talk.politics.guns                 364         314          50         140       0.863       0.692       0.768\n",
      "Total                            7,531       5,978       1,553       1,553\n",
      "Accuracy                                                                         0.794\n",
      "Micro Average                                                                    0.794       0.794       0.794\n",
      "Macro Average                                                                    0.787       0.790       0.787\n",
      "Balanced Error Rate                                                              0.213\n"
     ]
    }
   ],
   "source": [
    "var unigramStartTime = System.currentTimeMillis();\n",
    "var unigramModel = lrTrainer.train(unigramPair.getA());\n",
    "var unigramEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on Unigram features took \" + Util.formatDuration(unigramStartTime,unigramEndTime));\n",
    "System.out.println();\n",
    "var unigramEval = labelEvaluator.evaluate(unigramModel,unigramPair.getB());\n",
    "System.out.println(unigramEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the logistic regression trained on unigrams gets about 79% accuracy, pretty much the same as the BoW baseline, and takes about the same amount of time to run. Both of these make sense, as the term count isn't necessarily that useful in this particular dataset, and we didn't change the number of features overall or inside each example by using term counting.\n",
    "\n",
    "\n",
    "## N-grams as features\n",
    "Let's try a little more complicated feature extractor. The natural step from unigrams is to include word pairs (or bigrams) and count the occurrence of those. This allows us to get simple negations (e.g., \"not bad\" rather than \"not\" and \"bad\") along with places like \"New York\" rather than \"new\" and \"york\". In Tribuo this is as straightforward as telling the token pipeline we'd like bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram training data size = 11314, number of features = 1253665, number of classes = 20\n",
      "bigram testing data size = 7531, number of features = 1253665, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "var bigramPipeline = new TokenPipeline(tokenizer, 2, true);\n",
    "var bigramExtractor = new TextFeatureExtractorImpl<Label>(bigramPipeline);\n",
    "var bigramPair = mkDatasets(\"bigram\",bigramExtractor);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the feature space has massively increased due to the presence of bigram features, we've now got 1.2 million features from the same 11,314 documents.\n",
    "\n",
    "Now to train another logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on Bigram features took (00:00:44:572)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         347          51          88       0.872       0.798       0.833\n",
      "rec.autos                          396         313          83          36       0.790       0.897       0.840\n",
      "talk.religion.misc                 251         161          90         117       0.641       0.579       0.609\n",
      "comp.windows.x                     394         298          96          54       0.756       0.847       0.799\n",
      "rec.sport.baseball                 397         351          46          59       0.884       0.856       0.870\n",
      "comp.graphics                      389         292          97         167       0.751       0.636       0.689\n",
      "talk.politics.mideast              376         278          98          26       0.739       0.914       0.818\n",
      "comp.sys.ibm.pc.hardware           392         265         127         135       0.676       0.663       0.669\n",
      "sci.med                            396         277         119          62       0.699       0.817       0.754\n",
      "comp.os.ms-windows.misc            394         243         151          68       0.617       0.781       0.689\n",
      "sci.crypt                          396         341          55          56       0.861       0.859       0.860\n",
      "comp.sys.mac.hardware              385         314          71         189       0.816       0.624       0.707\n",
      "misc.forsale                       390         341          49          74       0.874       0.822       0.847\n",
      "rec.motorcycles                    398         363          35          42       0.912       0.896       0.904\n",
      "talk.politics.misc                 310         179         131          57       0.577       0.758       0.656\n",
      "sci.electronics                    393         251         142          84       0.639       0.749       0.690\n",
      "rec.sport.hockey                   399         372          27          38       0.932       0.907       0.920\n",
      "sci.space                          394         353          41         117       0.896       0.751       0.817\n",
      "alt.atheism                        319         222          97          67       0.696       0.768       0.730\n",
      "talk.politics.guns                 364         311          53         123       0.854       0.717       0.779\n",
      "Total                            7,531       5,872       1,659       1,659\n",
      "Accuracy                                                                         0.780\n",
      "Micro Average                                                                    0.780       0.780       0.780\n",
      "Macro Average                                                                    0.774       0.782       0.774\n",
      "Balanced Error Rate                                                              0.226\n"
     ]
    }
   ],
   "source": [
    "var bigramStartTime = System.currentTimeMillis();\n",
    "var bigramModel = lrTrainer.train(bigramPair.getA());\n",
    "var bigramEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on Bigram features took \" + Util.formatDuration(bigramStartTime,bigramEndTime));\n",
    "System.out.println();\n",
    "var bigramEval = labelEvaluator.evaluate(bigramModel,bigramPair.getB());\n",
    "System.out.println(bigramEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance decreased a little when using bigrams to 78%, and the runtime increased from 10s to 48s. This is because despite there being more information in the features, there are also many, many more features making it easier to confuse this simple linear model and each example takes longer to process due to the increased number of features. We could look at using a more complex model like boosted trees to exploit this additional information but as we increase the number of n-gram features we'll start to see diminishing returns as the model complexity increases without a commensurate increase in training data.\n",
    "\n",
    "## TFIDF vectors\n",
    "\n",
    "One other factor is that the count of some words isn't usually that helpful, most documents include \"a\", \"the\", \"and\" many times. A popular way to deal with this is to scale the term frequencies (i.e. the n-gram counts) by the Inverse Document Frequency (or IDF), producing TF-IDF vectors. In Tribuo the IDF is a transformation which is applied separately to the dataset after it's constructed, as it uses aggregate information from the whole dataset which isn't available until all the examples have been loaded in. Let's see how that affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf training data size = 11314, number of features = 1253665, number of classes = 20\n",
      "tf-idf testing data size = 7531, number of features = 335067, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "// Create a transformation map that contains a single IDFTransformation to apply to every feature\n",
    "var trMap = new TransformationMap(Collections.singletonList(new IDFTransformation()));\n",
    "// Copy out the datasets.\n",
    "var tfidfTrain = MutableDataset.createDeepCopy(bigramPair.getA());\n",
    "var tfidfTest = MutableDataset.createDeepCopy(bigramPair.getB());\n",
    "// Fit the IDF transformation and apply it to the data\n",
    "// We add the implicit zero features (i.e. the words not present in each document)\n",
    "// to get the correct estimate of the IDF.\n",
    "var transformers = tfidfTrain.createTransformers(trMap,true);\n",
    "tfidfTrain.transform(transformers);\n",
    "tfidfTest.transform(transformers);\n",
    "// Print the dataset statistics    \n",
    "System.out.println(String.format(\"tf-idf training data size = %d, number of features = %d, number of classes = %d\",tfidfTrain.size(),tfidfTrain.getFeatureMap().size(),tfidfTrain.getOutputInfo().size()));\n",
    "System.out.println(String.format(\"tf-idf testing data size = %d, number of features = %d, number of classes = %d\",tfidfTest.size(),tfidfTest.getFeatureMap().size(),tfidfTest.getOutputInfo().size()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating TF-IDF vectors didn't change the number of features, we still have 1.2 million features in the training set, but it has made the values of those features more informative, as we can see when we train a classifier on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on TF-IDF features took (00:00:48:502)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         350          48         106       0.879       0.768       0.820\n",
      "rec.autos                          396         326          70          60       0.823       0.845       0.834\n",
      "talk.religion.misc                 251         150         101          82       0.598       0.647       0.621\n",
      "comp.windows.x                     394         303          91          75       0.769       0.802       0.785\n",
      "rec.sport.baseball                 397         330          67          39       0.831       0.894       0.862\n",
      "comp.graphics                      389         274         115         132       0.704       0.675       0.689\n",
      "talk.politics.mideast              376         273         103          15       0.726       0.948       0.822\n",
      "comp.sys.ibm.pc.hardware           392         272         120         140       0.694       0.660       0.677\n",
      "sci.med                            396         282         114          80       0.712       0.779       0.744\n",
      "comp.os.ms-windows.misc            394         250         144          85       0.635       0.746       0.686\n",
      "sci.crypt                          396         341          55          42       0.861       0.890       0.875\n",
      "comp.sys.mac.hardware              385         254         131          43       0.660       0.855       0.745\n",
      "misc.forsale                       390         339          51          83       0.869       0.803       0.835\n",
      "rec.motorcycles                    398         360          38          50       0.905       0.878       0.891\n",
      "talk.politics.misc                 310         190         120         134       0.613       0.586       0.599\n",
      "sci.electronics                    393         300          93         139       0.763       0.683       0.721\n",
      "rec.sport.hockey                   399         374          25          58       0.937       0.866       0.900\n",
      "sci.space                          394         345          49          73       0.876       0.825       0.850\n",
      "alt.atheism                        319         256          63         114       0.803       0.692       0.743\n",
      "talk.politics.guns                 364         306          58         106       0.841       0.743       0.789\n",
      "Total                            7,531       5,875       1,656       1,656\n",
      "Accuracy                                                                         0.780\n",
      "Micro Average                                                                    0.780       0.780       0.780\n",
      "Macro Average                                                                    0.775       0.779       0.774\n",
      "Balanced Error Rate                                                              0.225\n"
     ]
    }
   ],
   "source": [
    "var tfidfStartTime = System.currentTimeMillis();\n",
    "var tfidfModel = lrTrainer.train(tfidfTrain);\n",
    "var tfidfEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on TF-IDF features took \" + Util.formatDuration(tfidfStartTime,tfidfEndTime));\n",
    "System.out.println();\n",
    "var tfidfEval = labelEvaluator.evaluate(tfidfModel,tfidfTest);\n",
    "System.out.println(tfidfEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF-IDF features has roughly the same accuracy as bigrams, so it may be that these features aren't something the linear model can easily operate on on this dataset, but in general the TF-IDF transformation is a useful one when working with text documents.\n",
    "\n",
    "## Feature hashing\n",
    "\n",
    "A popular technique for dealing with large feature spaces is feature hashing. This is where the features are mapped back down to a smaller space using a hash function. It induces collisions between the features, so the model might treat \"New York\" and \"San Francisco\" as the same feature, but the collisions are generated essentially at random based on the hash function which provides a strong regularising effect which can improves performance while making things run faster and in less memory.\n",
    "\n",
    "To use feature hashing in Tribuo simply pass a hash dimension to the `TokenPipeline` on construction. We'll map everything down to 50,000 features, which is only 5% of the original number and see how that affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash-100k training data size = 11314, number of features = 50000, number of classes = 20\n",
      "hash-100k testing data size = 7532, number of features = 50000, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "var hashPipeline = new TokenPipeline(tokenizer, 2, true, 50000);\n",
    "var hashExtractor = new TextFeatureExtractorImpl<Label>(hashPipeline);\n",
    "var hashPair = mkDatasets(\"hash-100k\",hashExtractor);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we have the same number of training & test examples, but now there are only 50,000 features. Let's build another logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on hashed features took (00:00:28:669)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         317          81          94       0.796       0.771       0.784\n",
      "rec.autos                          396         306          90         120       0.773       0.718       0.745\n",
      "talk.religion.misc                 251         141         110         110       0.562       0.562       0.562\n",
      "comp.windows.x                     395         278         117          77       0.704       0.783       0.741\n",
      "rec.sport.baseball                 397         334          63          93       0.841       0.782       0.811\n",
      "comp.graphics                      389         249         140         205       0.640       0.548       0.591\n",
      "talk.politics.mideast              376         302          74          83       0.803       0.784       0.794\n",
      "comp.sys.ibm.pc.hardware           392         249         143         183       0.635       0.576       0.604\n",
      "sci.med                            396         273         123         170       0.689       0.616       0.651\n",
      "comp.os.ms-windows.misc            394         229         165         102       0.581       0.692       0.632\n",
      "sci.crypt                          396         310          86          63       0.783       0.831       0.806\n",
      "comp.sys.mac.hardware              385         260         125         112       0.675       0.699       0.687\n",
      "misc.forsale                       390         311          79          81       0.797       0.793       0.795\n",
      "rec.motorcycles                    398         318          80          73       0.799       0.813       0.806\n",
      "talk.politics.misc                 310         163         147         117       0.526       0.582       0.553\n",
      "sci.electronics                    393         216         177         113       0.550       0.657       0.598\n",
      "rec.sport.hockey                   399         343          56          40       0.860       0.896       0.877\n",
      "sci.space                          394         296          98          69       0.751       0.811       0.780\n",
      "alt.atheism                        319         213         106          86       0.668       0.712       0.689\n",
      "talk.politics.guns                 364         296          68         137       0.813       0.684       0.743\n",
      "Total                            7,532       5,404       2,128       2,128\n",
      "Accuracy                                                                         0.717\n",
      "Micro Average                                                                    0.717       0.717       0.717\n",
      "Macro Average                                                                    0.712       0.716       0.712\n",
      "Balanced Error Rate                                                              0.288\n"
     ]
    }
   ],
   "source": [
    "var hashStartTime = System.currentTimeMillis();\n",
    "var hashModel = lrTrainer.train(hashPair.getA());\n",
    "var hashEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on hashed features took \" + Util.formatDuration(hashStartTime,hashEndTime));\n",
    "System.out.println();\n",
    "var hashEval = labelEvaluator.evaluate(hashModel,hashPair.getB());\n",
    "System.out.println(hashEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance dropped a little here, but the model has less than a tenth of the parameters compared to the bigram model, making it faster and much smaller at inference time, and it took around 66% of the time to train. In many cases dropping a couple of points of accuracy for a model that is 20x smaller and substantially faster is a worthwhile tradeoff, but as with most machine learning tasks this depends on the problem and where you're deploying the model. Tuning the hashing dimension and the trainer parameters will likely produce a model with similar accuracy at greatly reduced computational cost.\n",
    "\n",
    "## Trimming out infrequent features\n",
    "\n",
    "We can also directly trim out infrequently occuring features. If a feature doesn't occur very frequently then we're not likely to estimate it's weights properly as we've not seen it very often. Then if it occurs frequently in the test dataset it can confuse the model (this is a form of overfitting to the training data). Let's take the TF-IDF dataset and remove all the bigrams that occur fewer than 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum cardinality training data size = 11314, number of features = 114051, number of classes = 20\n",
      "Minimum cardinality testing data size = 7531, number of features = 114051, number of classes = 20\n"
     ]
    }
   ],
   "source": [
    "var minCardTrain = new MinimumCardinalityDataset<>(tfidfTrain,5);\n",
    "// This call creates a copy of bigramTest, removing all the \n",
    "// features not found in bigramTrain's feature and output maps\n",
    "var minCardTest = ImmutableDataset.copyDataset(tfidfTest,minCardTrain.getFeatureIDMap(),minCardTrain.getOutputIDInfo());\n",
    "// Print the dataset statistics    \n",
    "System.out.println(String.format(\"Minimum cardinality training data size = %d, number of features = %d, number of classes = %d\",minCardTrain.size(),minCardTrain.getFeatureMap().size(),minCardTrain.getOutputInfo().size()));\n",
    "System.out.println(String.format(\"Minimum cardinality testing data size = %d, number of features = %d, number of classes = %d\",minCardTest.size(),minCardTest.getFeatureMap().size(),minCardTest.getOutputInfo().size()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that's removed about 90% of the features, so let's try our simple model on it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on trimmed TF-IDF features took (00:00:21:885)\n",
      "\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         345          53         109       0.867       0.760       0.810\n",
      "rec.autos                          396         316          80          76       0.798       0.806       0.802\n",
      "talk.religion.misc                 251         162          89         130       0.645       0.555       0.597\n",
      "comp.windows.x                     394         309          85         116       0.784       0.727       0.755\n",
      "rec.sport.baseball                 397         347          50          62       0.874       0.848       0.861\n",
      "comp.graphics                      389         281         108         148       0.722       0.655       0.687\n",
      "talk.politics.mideast              376         291          85          20       0.774       0.936       0.847\n",
      "comp.sys.ibm.pc.hardware           392         250         142         141       0.638       0.639       0.639\n",
      "sci.med                            396         284         112          80       0.717       0.780       0.747\n",
      "comp.os.ms-windows.misc            394         239         155          92       0.607       0.722       0.659\n",
      "sci.crypt                          396         328          68          35       0.828       0.904       0.864\n",
      "comp.sys.mac.hardware              385         282         103          96       0.732       0.746       0.739\n",
      "misc.forsale                       390         332          58          84       0.851       0.798       0.824\n",
      "rec.motorcycles                    398         355          43          41       0.892       0.896       0.894\n",
      "talk.politics.misc                 310         177         133          96       0.571       0.648       0.607\n",
      "sci.electronics                    393         289         104         143       0.735       0.669       0.701\n",
      "rec.sport.hockey                   399         359          40          23       0.900       0.940       0.919\n",
      "sci.space                          394         307          87          37       0.779       0.892       0.832\n",
      "alt.atheism                        319         232          87          84       0.727       0.734       0.731\n",
      "talk.politics.guns                 364         308          56         125       0.846       0.711       0.773\n",
      "Total                            7,531       5,793       1,738       1,738\n",
      "Accuracy                                                                         0.769\n",
      "Micro Average                                                                    0.769       0.769       0.769\n",
      "Macro Average                                                                    0.764       0.768       0.764\n",
      "Balanced Error Rate                                                              0.236\n"
     ]
    }
   ],
   "source": [
    "var minCardStartTime = System.currentTimeMillis();\n",
    "var minCardModel = lrTrainer.train(minCardTrain);\n",
    "var minCardEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training the model on trimmed TF-IDF features took \" + Util.formatDuration(minCardStartTime,minCardEndTime));\n",
    "System.out.println();\n",
    "var minCardEval = labelEvaluator.evaluate(minCardModel,minCardTest);\n",
    "System.out.println(minCardEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the feature hashing above, this model trains more quickly because there is less data to process, but the speed improvement is more stubstantial as the number of features in each example is lower (because the hashing produces a denser example than trimming out infrequent features). Performance dropped slightly as compared to the TF-IDF model, but again it is around 10% of the parameters, with a corresponding reduction in memory and runtime in inference and training.\n",
    "\n",
    "Choosing which one of feature hashing and trimming out infrequent features to apply is problem dependent. Feature hashing can work in denser feature spaces than trimming infrequent features, but both still require some amount of sparsity in the problem to have any useful effect. With text datasets then trimming the infrequent words/features is usually helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "All the approaches described above have no notion of word similarity, they rely upon exactly the same words with the same spelling appearing in the training and test documents, when in practice word similarity is likely to be very useful information for the classifier because no two documents use exactly the same phrasing. For example, the unigrams \"excellent\" and \"fantastic\" are equally dissimilar to an n-gram model, when in fact those words are quite similar in meaning. Adding notions of word similarity to ML models usually means embedding each word into some vector space, then words with similar meanings can be close in the vector space, and words with dissimilar or opposite meanings are far apart. There are many popular word embedding algorithms, like [Word2Vec](https://arxiv.org/abs/1301.3781), [GloVe](https://nlp.stanford.edu/projects/glove/) or [FastText](https://fasttext.cc/) which build embeddings on a corpus of text that can then be used in downstream tasks. Tribuo doesn't have a class which can directly load those word vectors, as they all come in different file formats, but it's pretty straightforward to build a `TextFeatureExtractor` that will tokenize the input text, look up each word or n-gram in the vector space and then average them across the input (it took us about an afternoon to build one for our internal word2vec style word vector file format). If there is interest from the community in supporting a specific word vector file format, we're happy to accept PRs that add the support.\n",
    "\n",
    "While these more traditional forms of word vector are very powerful, as they are precomputed they treat each word the same no matter the context it appears in. For example \"bank\" could mean a river bank, or a financial institution, but a word2vec vector has both meanings because it doesn't have the *context* the word is present in, i.e., the rest of the sentence. This led to the rise of *contextual* word embeddings, which produce a vector for each word based on the whole input sequence. The most popular of these embeddings are based on the [Transformer](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) architecture, usually a variant of Google's [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) model.\n",
    "\n",
    "## Using BERT embeddings\n",
    "\n",
    "BERT is a multilayer transformer network, which reads in a sentence and produces both an embedding of the sentence, along with embeddings for each wordpiece. A wordpiece is the token that BERT operates on, which is either a whole word, or a chunk of a word, emitted by the wordpiece tokenizer. This word chunking is trained on a large corpus and allows common prefixes & suffixes (e.g. \"un\", \"ing\") to be split off the words and to share state. We can use BERT to produce a single vector which represents the sentence or document and then use that vector as features in a downstream Tribuo classifier.\n",
    "\n",
    "Tribuo works with BERT models that are stored in [ONNX format](https://onnx.ai), and can load in tokenizers produced by [HuggingFace Transformers](https://huggingface.co/transformers/) which helpfully provides a Python script to convert BERT models from HuggingFace format into ONNX format for deployment. We provide a `TextFeatureExtractor` implementation called `BERTFeatureExtractor` which can produce sentence embeddings out by passing the text through a BERT model. Tribuo uses Microsoft's [ONNX Runtime](https://www.onnxruntime.ai/) to load the model, and has it's own implementation of the Wordpiece tokenization algorithm, along with the necessary glue to produce tokens in the format that BERT expects. One downside of BERT models is that they have a maximum document length that they can process, usually 512 wordpieces. This is configurable in Tribuo's extractor, but if you set the maximum length to be longer than the sequences the model was trained on then the performance is likely to suffer (or the computation may fail depending on how that specific BERT model is specified).\n",
    "\n",
    "To follow along with this part of the tutorial you'll need to produce a BERT model in onnx format. To do that you'll need access to a Python 3 environment with HuggingFace and pytorch or TensorFlow installed to export the model (the snippet below assumes pytorch, change the `pt` to `tf` if you're using TensorFlow). Running the following snippet will produce a `bert-base-uncased.onnx` file that we can use for the rest of the tutorial. You'll need to run it in an empty directory due to the way HuggingFace's conversion script works.\n",
    "\n",
    "```\n",
    "python /path/to/huggingface/transformers/folder/convert_graph_to_onnx.py --framework pt --model bert-base-uncased bert-base-uncased.onnx\n",
    "```\n",
    "\n",
    "You'll also need to download the `tokenizer.json` that goes with the BERT variant you are using, for `bert-base-uncased` that file is [here](https://huggingface.co/bert-base-uncased/blob/main/tokenizer.json). Assuming both of those files are now in the same directory as this tutorial, we can create the `BERTFeatureExtractor`. We're going to start with just the `[CLS]` token which provides an embedding for the whole sentence.\n",
    "\n",
    "Warning: this feature extraction step took more than a minute per newsgroup on a 2019 16\" 6-core MacBook Pro (using the default settings of ONNX Runtime on the CPU provider) so around 55 minutes to extract the full train and test datasets. Your mileage may vary, and your laptop may get quite warm. The session options used can be controlled by the `BERTFeatureExtractor.reconfigureOrtSession(SessionOptions options)` method, which allows the use of whatever configuration is supported by your onnxruntime jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-cls training data size = 11314, number of features = 768, number of classes = 20\n",
      "bert-cls testing data size = 7532, number of features = 768, number of classes = 20\n",
      "Extracting CLS features with BERT took (00:58:06:984)\n"
     ]
    }
   ],
   "source": [
    "var bertPath = Paths.get(\"./bert-base-uncased.onnx\");\n",
    "var tokenizerPath = Paths.get(\"./tokenizer.json\");\n",
    "var bertCLS = new BERTFeatureExtractor<>(labelFactory,\n",
    "                                       bertPath,\n",
    "                                       tokenizerPath,\n",
    "                                       BERTFeatureExtractor.OutputPooling.CLS,\n",
    "                                       256, // maximum number of wordpiece tokens to use\n",
    "                                       false // Use Nvidia GPUs for BERT inference if available\n",
    "                                       );\n",
    "var clsStartTime = System.currentTimeMillis();\n",
    "var bertCLSPair = mkDatasets(\"bert-cls\",bertCLS);\n",
    "var clsEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Extracting CLS features with BERT took \" + Util.formatDuration(clsStartTime,clsEndTime));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Tribuo's `BERTFeatureExtractor` can run the BERT embeddings on a GPU, but only if the onnxruntime_gpu jar is on the classpath. By default Tribuo pulls in the CPU only jar for maximum compatibility. As you can see from the time taken to extract the features, it's best to deploy BERT when you've got plenty of CPUs or fast GPUs.\n",
    "\n",
    "Now we build a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LR on BERT features took (00:00:03:583)\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         353          45         229       0.887       0.607       0.720\n",
      "rec.autos                          396         338          58         787       0.854       0.300       0.444\n",
      "talk.religion.misc                 251           0         251           0       0.000       0.000       0.000\n",
      "comp.windows.x                     395         259         136         430       0.656       0.376       0.478\n",
      "rec.sport.baseball                 397         354          43         193       0.892       0.647       0.750\n",
      "comp.graphics                      389          13         376           7       0.033       0.650       0.064\n",
      "talk.politics.mideast              376         254         122          29       0.676       0.898       0.771\n",
      "comp.sys.ibm.pc.hardware           392         235         157         519       0.599       0.312       0.410\n",
      "sci.med                            396         240         156          33       0.606       0.879       0.717\n",
      "comp.os.ms-windows.misc            394          78         316          56       0.198       0.582       0.295\n",
      "sci.crypt                          396         299          97         402       0.755       0.427       0.545\n",
      "comp.sys.mac.hardware              385         150         235         280       0.390       0.349       0.368\n",
      "misc.forsale                       390         240         150          69       0.615       0.777       0.687\n",
      "rec.motorcycles                    398          71         327          12       0.178       0.855       0.295\n",
      "talk.politics.misc                 310         158         152         320       0.510       0.331       0.401\n",
      "sci.electronics                    393          49         344           6       0.125       0.891       0.219\n",
      "rec.sport.hockey                   399         297         102          20       0.744       0.937       0.830\n",
      "sci.space                          394         267         127          97       0.678       0.734       0.704\n",
      "alt.atheism                        319         125         194         152       0.392       0.451       0.419\n",
      "talk.politics.guns                 364          67         297          44       0.184       0.604       0.282\n",
      "Total                            7,532       3,847       3,685       3,685\n",
      "Accuracy                                                                         0.511\n",
      "Micro Average                                                                    0.511       0.511       0.511\n",
      "Macro Average                                                                    0.499       0.580       0.470\n",
      "Balanced Error Rate                                                              0.501\n"
     ]
    }
   ],
   "source": [
    "var clsMStartTime = System.currentTimeMillis();\n",
    "var clsModel = lrTrainer.train(bertCLSPair.getA());\n",
    "var clsMEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training a LR on BERT features took \" + Util.formatDuration(clsMStartTime,clsMEndTime));\n",
    "var clsEval = labelEvaluator.evaluate(clsModel,bertCLSPair.getB());\n",
    "System.out.println(clsEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLS token usually needs to be fine-tuned on the specific task, so it's not too surprising that it's performance is lacking, but you can load a fine-tuned BERT model into Tribuo if one is available for your task. We can also use the average of the token embeddings as the document vector and see if that improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-ave training data size = 11314, number of features = 768, number of classes = 20\n",
      "bert-ave testing data size = 7532, number of features = 768, number of classes = 20\n",
      "Extracting Mean features with BERT took (00:38:31:380)\n"
     ]
    }
   ],
   "source": [
    "var bertAve = new BERTFeatureExtractor<>(labelFactory,\n",
    "                                       bertPath,\n",
    "                                       tokenizerPath,\n",
    "                                       BERTFeatureExtractor.OutputPooling.MEAN,\n",
    "                                       256, // Maximum number of wordpiece tokens\n",
    "                                       false // Use Nvidia GPUs for inference if available\n",
    "                                      );\n",
    "                                      \n",
    "var aveStartTime = System.currentTimeMillis();\n",
    "var bertAvePair = mkDatasets(\"bert-ave\",bertAve);\n",
    "var aveEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Extracting Mean features with BERT took \" + Util.formatDuration(aveStartTime,aveEndTime));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a LR on BERT features took (00:00:03:397)\n",
      "Class                                n          tp          fn          fp      recall        prec          f1\n",
      "soc.religion.christian             398         365          33         149       0.917       0.710       0.800\n",
      "rec.autos                          396         325          71          66       0.821       0.831       0.826\n",
      "talk.religion.misc                 251          82         169          94       0.327       0.466       0.384\n",
      "comp.windows.x                     395         252         143          59       0.638       0.810       0.714\n",
      "rec.sport.baseball                 397         370          27          33       0.932       0.918       0.925\n",
      "comp.graphics                      389         233         156         159       0.599       0.594       0.597\n",
      "talk.politics.mideast              376         318          58          43       0.846       0.881       0.863\n",
      "comp.sys.ibm.pc.hardware           392         265         127         285       0.676       0.482       0.563\n",
      "sci.med                            396         344          52          88       0.869       0.796       0.831\n",
      "comp.os.ms-windows.misc            394         244         150         189       0.619       0.564       0.590\n",
      "sci.crypt                          396         286         110          45       0.722       0.864       0.787\n",
      "comp.sys.mac.hardware              385         211         174         118       0.548       0.641       0.591\n",
      "misc.forsale                       390         303          87          38       0.777       0.889       0.829\n",
      "rec.motorcycles                    398         285         113          27       0.716       0.913       0.803\n",
      "talk.politics.misc                 310         138         172          89       0.445       0.608       0.514\n",
      "sci.electronics                    393         239         154         152       0.608       0.611       0.610\n",
      "rec.sport.hockey                   399         375          24          17       0.940       0.957       0.948\n",
      "sci.space                          394         338          56         109       0.858       0.756       0.804\n",
      "alt.atheism                        319         153         166          78       0.480       0.662       0.556\n",
      "talk.politics.guns                 364         309          55         259       0.849       0.544       0.663\n",
      "Total                            7,532       5,435       2,097       2,097\n",
      "Accuracy                                                                         0.722\n",
      "Micro Average                                                                    0.722       0.722       0.722\n",
      "Macro Average                                                                    0.709       0.725       0.710\n",
      "Balanced Error Rate                                                              0.291\n"
     ]
    }
   ],
   "source": [
    "var aveMStartTime = System.currentTimeMillis();\n",
    "var aveModel = lrTrainer.train(bertAvePair.getA());\n",
    "var aveMEndTime = System.currentTimeMillis();\n",
    "System.out.println(\"Training a LR on BERT features took \" + Util.formatDuration(aveMStartTime,aveMEndTime));\n",
    "var aveEval = labelEvaluator.evaluate(aveModel,bertAvePair.getB());\n",
    "System.out.println(aveEval);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using BERT ...\n",
    "\n",
    "Using different BERT versions can change the accuracy, and there are smaller versions like DistillBERT and TinyBERT which are useful for deploying models in constrained environments, though these models will always be slower than the simpler BoW approaches described above.\n",
    "\n",
    "# Deploying the feature extractors\n",
    "\n",
    "Similarly to when working with columnar data, the feature extractor used is recorded in the model provenance. We can see that for the BERT model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectoryFileSource(\n",
      "\tclass-name = org.tribuo.data.text.DirectoryFileSource\n",
      "\tdataDir = /Users/apocock/Development/Tribuo/tutorials/20news/20news-bydate-train\n",
      "\tpreprocessors = List[\n",
      "\t\tNewsPreprocessor(\n",
      "\t\t\t\t\tclass-name = org.tribuo.data.text.impl.NewsPreprocessor\n",
      "\t\t\t\t\thost-short-name = DocumentPreprocessor\n",
      "\t\t\t\t)\n",
      "\t]\n",
      "\textractor = BERTFeatureExtractor(\n",
      "\t\t\tclass-name = org.tribuo.interop.onnx.extractors.BERTFeatureExtractor\n",
      "\t\t\tuseCUDA = false\n",
      "\t\t\tpooling = CLS_AND_MEAN\n",
      "\t\t\tmodelPath = /Users/apocock/Development/Tribuo/tutorials/bert-base-uncased.onnx\n",
      "\t\t\ttokenizerPath = /Users/apocock/Development/Tribuo/tutorials/tokenizer.json\n",
      "\t\t\toutputFactory = LabelFactory(\n",
      "\t\t\t\t\tclass-name = org.tribuo.classification.LabelFactory\n",
      "\t\t\t\t)\n",
      "\t\t\tmaxLength = 512\n",
      "\t\t\thost-short-name = FeatureExtractor\n",
      "\t\t)\n",
      "\toutputFactory = LabelFactory(\n",
      "\t\t\tclass-name = org.tribuo.classification.LabelFactory\n",
      "\t\t)\n",
      "\tfile-modified-time = 2003-03-18T07:24:55-05:00\n",
      "\tdatasource-creation-time = 2021-04-19T19:19:56.733374-04:00\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "var sourceProvenance = aveModel.getProvenance().getDatasetProvenance().getSourceProvenance();\n",
    "System.out.println(ProvenanceUtil.formattedProvenanceString(sourceProvenance));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the model has recorded how the features were extracted, but the extraction process itself isn't part of the serialized model (which we wouldn't really want anyway as BERT models are hundreds of megabytes). So to use one of these models at inference time the feature extraction pipeline needs to be rebuilt from the configuration, in the same way we rebuilt the `RowProcessor` in the columnar tutorial.\n",
    "\n",
    "Each of the different models trained in this tutorial has recorded the source provenance and it's associated `TextFeatureExtractor` configuration, meaning the models come with all the necessary information to infer the classes of new documents.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "We looked at a document classification task in Tribuo. As most of the work in NLP tends to be on featurising the data, we discussed several different ways of converting text into features for use in machine learning. We looked at Bag of Words models, using n-grams, term frequencies, TFIDF vectors, feature hashing and also looked at trimming large feature spaces based on the number of times we'd seen a feature. We also discussed word vector approaches, and showed how to use the popular contextual word embedding model, BERT, to extract features for document classification. It's worth noting all the models trained were simple logistic regressions, with no parameter tuning. Using a more powerful classifier like XGBoost, or performing hyperparameter tuning on the logistic regression will likely improve performance over the simple baselines presented here.\n",
    "\n",
    "Tribuo's text processing framework is very flexible, and it's possible to insert your own code into each of the different classes by implementing `TextFeatureExtractor`, `TextPipeline` or even the `Tokenizer` yourself, while the provenance system ensures that you can always recover how your data was processed to ensure it matches at inference time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "16+36-2231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
